
            corpus:
                train_corpus_loc: data/topic10_word100_long.train
                dev_corpus_loc: data/topic10_word100_long.dev
                test_corpus_loc:  data/topic10_word100_long.test
            language:
                name: lda
                num_topics: 10
                num_words: 100
                vocab:
                          'PAD': 0
                          'MASK': 1
                          'START': 2
                          'END': 3
                          '0': 4
                          '1': 5
                          '2': 6
                          '3': 7
                          '4': 8
                          '5': 9
                          '6': 10
                          '7': 11
                          '8': 12
                          '9': 13
                          '10': 14
                          '11': 15
                          '12': 16
                          '13': 17
                          '14': 18
                          '15': 19
                          '16': 20
                          '17': 21
                          '18': 22
                          '19': 23
                          '20': 24
                          '21': 25
                          '22': 26
                          '23': 27
                          '24': 28
                          '25': 29
                          '26': 30
                          '27': 31
                          '28': 32
                          '29': 33
                          '30': 34
                          '31': 35
                          '32': 36
                          '33': 37
                          '34': 38
                          '35': 39
                          '36': 40
                          '37': 41
                          '38': 42
                          '39': 43
                          '40': 44
                          '41': 45
                          '42': 46
                          '43': 47
                          '44': 48
                          '45': 49
                          '46': 50
                          '47': 51
                          '48': 52
                          '49': 53
                          '50': 54
                          '51': 55
                          '52': 56
                          '53': 57
                          '54': 58
                          '55': 59
                          '56': 60
                          '57': 61
                          '58': 62
                          '59': 63
                          '60': 64
                          '61': 65
                          '62': 66
                          '63': 67
                          '64': 68
                          '65': 69
                          '66': 70
                          '67': 71
                          '68': 72
                          '69': 73
                          '70': 74
                          '71': 75
                          '72': 76
                          '73': 77
                          '74': 78
                          '75': 79
                          '76': 80
                          '77': 81
                          '78': 82
                          '79': 83
                          '80': 84
                          '81': 85
                          '82': 86
                          '83': 87
                          '84': 88
                          '85': 89
                          '86': 90
                          '87': 91
                          '88': 92
                          '89': 93
                          '90': 94
                          '91': 95
                          '92': 96
                          '93': 97
                          '94': 98
                          '95': 99
                          '96': 100
                          '97': 101
                          '98': 102
                          '99': 103
                dev_sample_count:  10000
                test_sample_count: 10000
                train_sample_count: 10000
            lm:
                embedding_dim: 104
                hidden_dim: 104
                lm_type: BertForMaskedLMCustom
                residual: False  # TODO whether the self attention has residual connections
                attn_output_fc: False  # TODO whether the self attention output has a fully connected layer
                bert_intermediate: False  # TODO whether the BertLayer has a BertIntermediate (FC) sub-layer
                bert_output: False  # TODO whether the BertLayer has a BertOutput (FC with residual) sub-layer
                bert_head_transform: False  # whether the BertLMPredictionHead has a transform (FC) sub-layer
                layer_norm: False  # whether the model has LayerNorm
                num_layers: 1
                save_path: lm.params
                num_heads: 1
                embedding_type: none
                token_embedding_type: one_hot  # trained or one_hot
                freeze_uniform_attention: True  # TODO freeze W^K and W^Q to 0 
                freeze_id_value_matrix: False  # TODO freeze W^V to I
                freeze_block_value_matrix: False  # TODO
                freeze_decoder_to_I: True
                no_softmax: False  # remove the final softmax layer and change the loss to MSELoss
            reporting:
                reporting_loc: ./trained_models/lda_bert_simplified_one_hot/  # TODO
                reporting_methods:
                - constraints
                plot_attention_dir: ./plot_attention/lda_bert_simplified_one_hot/  # TODO
                inspect_results_dir: ./inspect_results/lda_bert_simplified_one_hot/  # TODO
                num_sentences_to_plot: 5
                random: False  # TODO 
                log_all_steps_until: 0  # log all the first several steps to wandb
            training:
                batch_size: 40
                dropout: 0.0
                optimizer: Adam  # Adam or SGD
                learning_rate: 0.01
                weight_decay: 0.0
                max_epochs: 20  # LIKELY TOO LOW, JUST A DEMO
                seed: 0
                objective: default  # default or contrastive or multi
                mask_prob: 0.15  # Should almost always be 0.0 for GPT
                mask_correct_prob: 0.1  # the proportion of "masked" tokens that show the correct token
                mask_random_prob: 0.1  # the proportion of "masked" tokens that show a random token
                zero_init_attn: False  # init W^K, W^Q, W^V to near 0
                zero_init_emb_dec: False  # init embedding and decoder to near 0
                zero_init_noise: 0.0  # noise for `near 0`
            experiment:
                repeat: 1  # number of times to re-train the model
            