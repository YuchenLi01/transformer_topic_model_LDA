{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS4CbKpkxmnu"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3EPhRgYqiTv"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put this notebook in the folder `grammar_compositionality/`"
      ],
      "metadata": {
        "id": "L0uLlElLVC2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LA2HNyCj22e"
      },
      "outputs": [],
      "source": [
        "!pip install -U wandb\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGL_FPpUj4Qv"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myL5N63Mj6H8"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data setting"
      ],
      "metadata": {
        "id": "2776BtKhuoa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qElSHE6SVBZd",
        "outputId": "ecd7f454-5963-41bf-f12c-665bd74cd2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic_term_matrix\n",
            "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n",
            "  0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1\n",
            "  0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1\n",
            "  0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1\n",
            "  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "num_words = 100\n",
        "num_topics = 10\n",
        "num_words_per_topic = num_words // num_topics\n",
        "topic_model = {}\n",
        "\n",
        "topic_term_matrix = np.zeros((num_topics, num_words))\n",
        "for i in range(num_topics):\n",
        "    words_of_topic_i = range(num_words_per_topic * i, num_words_per_topic * (i+1))\n",
        "    for word in words_of_topic_i:\n",
        "        topic_term_matrix[i][word] = num_topics / num_words\n",
        "        topic_model[word] = i\n",
        "\n",
        "print('topic_term_matrix')\n",
        "print(topic_term_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwxgf49grXzW"
      },
      "outputs": [],
      "source": [
        "num_train_sentences = 10000\n",
        "num_dev_sentences = 10000\n",
        "num_test_sentences = 10000\n",
        "sentence_len_min = 100 # 10 100\n",
        "sentence_len_max = 150 # 40 150\n",
        "alpha = [0.1] * num_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwRphJJ3S6I2"
      },
      "source": [
        "## Only run if need to re-generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W_TmtAaIe_g"
      },
      "outputs": [],
      "source": [
        "with open(f\"./trained_models/topic{num_topics}_word{num_words}.pkl\", 'wb') as f:\n",
        "    pickle.dump(topic_model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3AqFJ5eVYqH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from scipy.stats import dirichlet\n",
        "\n",
        "\n",
        "def write_lda_data(fn, num_sentences):\n",
        "    with open(fn, 'wt') as f:\n",
        "        for i in range(num_sentences):\n",
        "            sentence_len = random.randint(sentence_len_min, sentence_len_max)\n",
        "            topic_distr = dirichlet.rvs(alpha, size=None)\n",
        "            for _ in range(sentence_len):\n",
        "                topic = np.random.choice(range(num_topics), p=topic_distr)\n",
        "                word = np.random.choice(range(num_words), p=topic_term_matrix[topic])\n",
        "                f.write(f\"{word} \")\n",
        "            f.write(f\"END\\n\")\n",
        "\n",
        "write_lda_data(f\"data/topic{num_topics}_word{num_words}_long.train\", num_train_sentences)\n",
        "write_lda_data(f\"data/topic{num_topics}_word{num_words}_long.dev\", num_dev_sentences)\n",
        "write_lda_data(f\"data/topic{num_topics}_word{num_words}_long.test\", num_test_sentences)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate config"
      ],
      "metadata": {
        "id": "XQBRx6OlxvK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJEXOKOa6tR"
      },
      "outputs": [],
      "source": [
        "def gen_vocab_str(num_words):\n",
        "    vocab = dict(zip(['PAD', 'MASK', 'START', 'END'] + list(range(num_words)), range(num_words+4)))\n",
        "\n",
        "    vocab_str = 'vocab:'\n",
        "    for token in vocab:\n",
        "        vocab_str += f\"\\n{' ' * 26}'{token}': {vocab[token]}\"\n",
        "    return vocab_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wl0-uJ-kiXE",
        "outputId": "15b70a65-c531-4c68-b2ce-8a8b84ad24f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab:\n",
            "                          'PAD': 0\n",
            "                          'MASK': 1\n",
            "                          'START': 2\n",
            "                          'END': 3\n",
            "                          '0': 4\n",
            "                          '1': 5\n",
            "                          '2': 6\n",
            "                          '3': 7\n",
            "                          '4': 8\n",
            "                          '5': 9\n",
            "                          '6': 10\n",
            "                          '7': 11\n",
            "                          '8': 12\n",
            "                          '9': 13\n",
            "                          '10': 14\n",
            "                          '11': 15\n",
            "                          '12': 16\n",
            "                          '13': 17\n",
            "                          '14': 18\n",
            "                          '15': 19\n",
            "                          '16': 20\n",
            "                          '17': 21\n",
            "                          '18': 22\n",
            "                          '19': 23\n",
            "                          '20': 24\n",
            "                          '21': 25\n",
            "                          '22': 26\n",
            "                          '23': 27\n",
            "                          '24': 28\n",
            "                          '25': 29\n",
            "                          '26': 30\n",
            "                          '27': 31\n",
            "                          '28': 32\n",
            "                          '29': 33\n",
            "                          '30': 34\n",
            "                          '31': 35\n",
            "                          '32': 36\n",
            "                          '33': 37\n",
            "                          '34': 38\n",
            "                          '35': 39\n",
            "                          '36': 40\n",
            "                          '37': 41\n",
            "                          '38': 42\n",
            "                          '39': 43\n",
            "                          '40': 44\n",
            "                          '41': 45\n",
            "                          '42': 46\n",
            "                          '43': 47\n",
            "                          '44': 48\n",
            "                          '45': 49\n",
            "                          '46': 50\n",
            "                          '47': 51\n",
            "                          '48': 52\n",
            "                          '49': 53\n",
            "                          '50': 54\n",
            "                          '51': 55\n",
            "                          '52': 56\n",
            "                          '53': 57\n",
            "                          '54': 58\n",
            "                          '55': 59\n",
            "                          '56': 60\n",
            "                          '57': 61\n",
            "                          '58': 62\n",
            "                          '59': 63\n",
            "                          '60': 64\n",
            "                          '61': 65\n",
            "                          '62': 66\n",
            "                          '63': 67\n",
            "                          '64': 68\n",
            "                          '65': 69\n",
            "                          '66': 70\n",
            "                          '67': 71\n",
            "                          '68': 72\n",
            "                          '69': 73\n",
            "                          '70': 74\n",
            "                          '71': 75\n",
            "                          '72': 76\n",
            "                          '73': 77\n",
            "                          '74': 78\n",
            "                          '75': 79\n",
            "                          '76': 80\n",
            "                          '77': 81\n",
            "                          '78': 82\n",
            "                          '79': 83\n",
            "                          '80': 84\n",
            "                          '81': 85\n",
            "                          '82': 86\n",
            "                          '83': 87\n",
            "                          '84': 88\n",
            "                          '85': 89\n",
            "                          '86': 90\n",
            "                          '87': 91\n",
            "                          '88': 92\n",
            "                          '89': 93\n",
            "                          '90': 94\n",
            "                          '91': 95\n",
            "                          '92': 96\n",
            "                          '93': 97\n",
            "                          '94': 98\n",
            "                          '95': 99\n",
            "                          '96': 100\n",
            "                          '97': 101\n",
            "                          '98': 102\n",
            "                          '99': 103\n"
          ]
        }
      ],
      "source": [
        "print(gen_vocab_str(num_words))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hiddenlayers in [1]:\n",
        "  for num_heads in [1]:\n",
        "    for optimizer in ['Adam']:  # ['Adam', 'SGD']\n",
        "        for lr in [0.01]: \n",
        "            config_text = f\"\"\"\n",
        "            corpus:\n",
        "                train_corpus_loc: data/topic10_word100_long.train\n",
        "                dev_corpus_loc: data/topic10_word100_long.dev\n",
        "                test_corpus_loc:  data/topic10_word100_long.test\n",
        "            language:\n",
        "                name: lda\n",
        "                num_topics: {num_topics}\n",
        "                num_words: {num_words}\n",
        "                {gen_vocab_str(num_words)}\n",
        "                dev_sample_count:  {num_dev_sentences}\n",
        "                test_sample_count: {num_test_sentences}\n",
        "                train_sample_count: {num_train_sentences}\n",
        "            lm:\n",
        "                embedding_dim: {(num_words + 4)}\n",
        "                hidden_dim: {(num_words + 4)}\n",
        "                lm_type: BertForMaskedLMCustom\n",
        "                residual: False  # TODO whether the self attention has residual connections\n",
        "                attn_output_fc: False  # TODO whether the self attention output has a fully connected layer\n",
        "                bert_intermediate: False  # TODO whether the BertLayer has a BertIntermediate (FC) sub-layer\n",
        "                bert_output: False  # TODO whether the BertLayer has a BertOutput (FC with residual) sub-layer\n",
        "                bert_head_transform: False  # whether the BertLMPredictionHead has a transform (FC) sub-layer\n",
        "                layer_norm: False  # whether the model has LayerNorm\n",
        "                num_layers: {hiddenlayers}\n",
        "                save_path: lm.params\n",
        "                num_heads: {num_heads}\n",
        "                embedding_type: none\n",
        "                token_embedding_type: one_hot  # trained or one_hot\n",
        "                freeze_uniform_attention: True  # TODO freeze W^K and W^Q to 0 \n",
        "                freeze_id_value_matrix: False  # TODO freeze W^V to I\n",
        "                freeze_block_value_matrix: False  # TODO\n",
        "                freeze_decoder_to_I: True\n",
        "                no_softmax: False  # remove the final softmax layer and change the loss to MSELoss\n",
        "            reporting:\n",
        "                reporting_loc: ./trained_models/lda_bert_simplified_one_hot/  # TODO\n",
        "                reporting_methods:\n",
        "                - constraints\n",
        "                plot_attention_dir: ./plot_attention/lda_bert_simplified_one_hot/  # TODO\n",
        "                inspect_results_dir: ./inspect_results/lda_bert_simplified_one_hot/  # TODO\n",
        "                num_sentences_to_plot: 5\n",
        "                random: False  # TODO \n",
        "                log_all_steps_until: 0  # log all the first several steps to wandb\n",
        "            training:\n",
        "                batch_size: 40\n",
        "                dropout: 0.0\n",
        "                optimizer: {optimizer}  # Adam or SGD\n",
        "                learning_rate: {lr}\n",
        "                weight_decay: 0.0\n",
        "                max_epochs: 20  # LIKELY TOO LOW, JUST A DEMO\n",
        "                seed: 0\n",
        "                objective: default  # default or contrastive or multi\n",
        "                mask_prob: 0.15  # Should almost always be 0.0 for GPT\n",
        "                mask_correct_prob: 0.1  # the proportion of \"masked\" tokens that show the correct token\n",
        "                mask_random_prob: 0.1  # the proportion of \"masked\" tokens that show a random token\n",
        "                zero_init_attn: False  # init W^K, W^Q, W^V to near 0\n",
        "                zero_init_emb_dec: False  # init embedding and decoder to near 0\n",
        "                zero_init_noise: 0.0  # noise for `near 0`\n",
        "            experiment:\n",
        "                repeat: 1  # number of times to re-train the model\n",
        "            \"\"\"\n",
        "            with open(f\"config/bert_lda_hiddenlayers{hiddenlayers}_heads{num_heads}_lr{lr}_one_hot.yaml\", 'wt') as f:\n",
        "                f.write(config_text)"
      ],
      "metadata": {
        "id": "KD3OgWFExxGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3QMT19u4vig"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "518c-NjG87Bd",
        "outputId": "fd512da6-5fe2-4447-c7e4-211f2e383b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/bin/bash\n",
            "\n",
            "for hiddenlayers in 1\n",
            "do\n",
            "  for num_heads in 1\n",
            "  do\n",
            "    for lr in 0.01\n",
            "    do\n",
            "      python3 src/run_lm.py \"config/bert_lda_hiddenlayers\"$hiddenlayers\"_heads\"$num_heads\"_lr\"$lr\"_one_hot.yaml\"\n",
            "    done\n",
            "  done\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "!cat scripts/train_lda_bert.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBs8LT-LFoZk",
        "outputId": "2cb4a1b9-b91b-48df-80fd-d4de37fd921b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src/run_lm.py:23: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  args = yaml.load(open(config_file))\n",
            "Getting dataset from data/topic10_word100_long.train\n",
            "Getting dataset from data/topic10_word100_long.dev\n",
            "Getting dataset from data/topic10_word100_long.test\n",
            "Construct the language model with args {'corpus': {'train_corpus_loc': 'data/topic10_word100_long.train', 'dev_corpus_loc': 'data/topic10_word100_long.dev', 'test_corpus_loc': 'data/topic10_word100_long.test'}, 'language': {'name': 'lda', 'num_topics': 10, 'num_words': 100, 'vocab': {'PAD': 0, 'MASK': 1, 'START': 2, 'END': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, '10': 14, '11': 15, '12': 16, '13': 17, '14': 18, '15': 19, '16': 20, '17': 21, '18': 22, '19': 23, '20': 24, '21': 25, '22': 26, '23': 27, '24': 28, '25': 29, '26': 30, '27': 31, '28': 32, '29': 33, '30': 34, '31': 35, '32': 36, '33': 37, '34': 38, '35': 39, '36': 40, '37': 41, '38': 42, '39': 43, '40': 44, '41': 45, '42': 46, '43': 47, '44': 48, '45': 49, '46': 50, '47': 51, '48': 52, '49': 53, '50': 54, '51': 55, '52': 56, '53': 57, '54': 58, '55': 59, '56': 60, '57': 61, '58': 62, '59': 63, '60': 64, '61': 65, '62': 66, '63': 67, '64': 68, '65': 69, '66': 70, '67': 71, '68': 72, '69': 73, '70': 74, '71': 75, '72': 76, '73': 77, '74': 78, '75': 79, '76': 80, '77': 81, '78': 82, '79': 83, '80': 84, '81': 85, '82': 86, '83': 87, '84': 88, '85': 89, '86': 90, '87': 91, '88': 92, '89': 93, '90': 94, '91': 95, '92': 96, '93': 97, '94': 98, '95': 99, '96': 100, '97': 101, '98': 102, '99': 103}, 'dev_sample_count': 10000, 'test_sample_count': 10000, 'train_sample_count': 10000, 'vocab_size': 104}, 'lm': {'embedding_dim': 104, 'hidden_dim': 104, 'lm_type': 'BertForMaskedLMCustom', 'residual': False, 'attn_output_fc': False, 'bert_intermediate': False, 'bert_output': False, 'bert_head_transform': False, 'layer_norm': False, 'num_layers': 1, 'save_path': 'lm.params', 'num_heads': 1, 'embedding_type': 'none', 'token_embedding_type': 'one_hot', 'freeze_uniform_attention': True, 'freeze_id_value_matrix': False, 'freeze_block_value_matrix': False, 'freeze_decoder_to_I': True, 'no_softmax': False}, 'reporting': {'reporting_loc': './trained_models/lda_bert_simplified_one_hot/', 'reporting_methods': ['constraints'], 'plot_attention_dir': './plot_attention/lda_bert_simplified_one_hot/', 'inspect_results_dir': './inspect_results/lda_bert_simplified_one_hot/', 'num_sentences_to_plot': 5, 'random': False, 'log_all_steps_until': 0}, 'training': {'batch_size': 40, 'dropout': 0.0, 'optimizer': 'Adam', 'learning_rate': 0.01, 'weight_decay': 0.0, 'max_epochs': 20, 'seed': 0, 'objective': 'default', 'mask_prob': 0.15, 'mask_correct_prob': 0.1, 'mask_random_prob': 0.1, 'zero_init_attn': False, 'zero_init_emb_dec': False, 'zero_init_noise': 0.0}, 'experiment': {'repeat': 1}, 'name': 'topic10_word100_BertForMaskedLMCustom_Adam_lr0.01_wd0.0_hiddenlayers1_heads1_hiddendim104_none_one_hot_default_dropout0.0_noise0.0_mask_0.15_correct0.1_random0.1_noMany_freezeUniformAttention_freezeWdecI0', 'device': device(type='cuda', index=0)}\n",
            "config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"attn_output_fc\": false,\n",
            "  \"bert_head_transform\": false,\n",
            "  \"bert_intermediate\": false,\n",
            "  \"bert_output\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"freeze_block_value_matrix\": false,\n",
            "  \"freeze_decoder_to_I\": true,\n",
            "  \"freeze_id_value_matrix\": false,\n",
            "  \"freeze_uniform_attention\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 104,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 104,\n",
            "  \"layer_norm\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 6000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"residual\": false,\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 104,\n",
            "  \"zero_init_attn\": false,\n",
            "  \"zero_init_emb_dec\": false,\n",
            "  \"zero_init_noise\": 0.0\n",
            "}\n",
            "\n",
            "model BertForMaskedLMCustom(\n",
            "  (bert): BertModelCustom(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(104, 104, padding_idx=0)\n",
            "      (position_embeddings): Embedding(6000, 104)\n",
            "      (token_type_embeddings): Embedding(2, 104)\n",
            "      (LayerNorm): LayerNorm((104,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoderCustom(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayerCustom(\n",
            "          (attention): BertAttentionCustom(\n",
            "            (self): BertSelfAttentionCustom(\n",
            "              (query): Linear(in_features=104, out_features=104, bias=True)\n",
            "              (key): Linear(in_features=104, out_features=104, bias=True)\n",
            "              (value): Linear(in_features=104, out_features=104, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutputCustom(\n",
            "              (LayerNorm): Identity()\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): None\n",
            "          (output): None\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): None\n",
            "  )\n",
            "  (cls): BertOnlyMLMHeadCustom(\n",
            "    (predictions): BertLMPredictionHeadCustom(\n",
            "      (transform): None\n",
            "      (decoder): Linear(in_features=104, out_features=104, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Constructing a BertForMaskedLMCustom pytorch model w hidden size 104, layers 1, dropout 0.0\n",
            "Writing results to ./trained_models/lda_bert_simplified_one_hot/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuchenl4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/gdrive/MyDrive/research/project/hierarchical_compositionality/codes/grammar_compositionality_202302/wandb/run-20230208_075800-og1wm7nc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtopic10_word100_BertForMaskedLMCustom_Adam_lr0.01_wd0.0_hiddenlayers1_heads1_hiddendim104_none_one_hot_default_dropout0.0_noise0.0_mask_0.15_correct0.1_random0.1_noMany_freezeUniformAttention_freezeWdecI0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/yuchenl4/transformer_topic_modeling_learning_dynamics\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/yuchenl4/transformer_topic_modeling_learning_dynamics/runs/og1wm7nc\u001b[0m\n",
            "./trained_models/lda_bert_simplified_one_hot/topic10_word100_BertForMaskedLMCustom_Adam_lr0.01_wd0.0_hiddenlayers1_heads1_hiddendim104_none_one_hot_default_dropout0.0_noise0.0_mask_0.15_correct0.1_random0.1_noMany_freezeUniformAttention_freezeWdecI0.params\n",
            "[epoch 0] Train loss: 4.774789333343506, Dev loss: 4.799540361404419\n",
            "Saving lm parameters\n",
            "Min dev loss: 4.799540361404419\n",
            "Min dev loss: 4.799540361404419\n",
            "Min dev loss: 4.799540361404419\n",
            "Min dev loss: 4.799540361404419\n",
            "[epoch 4] Train loss: 3.5482325553894043, Dev loss: 3.3953701877593994\n",
            "Saving lm parameters\n",
            "Min dev loss: 3.3953701877593994\n",
            "Min dev loss: 3.3953701877593994\n",
            "Min dev loss: 3.3953701877593994\n",
            "Min dev loss: 3.3953701877593994\n",
            "Min dev loss: 3.3953701877593994\n",
            "Min dev loss: 3.3953701877593994\n",
            "Min dev loss: 3.3953701877593994\n",
            "Min dev loss: 3.3953701877593994\n",
            "Early stopping\n",
            "Min dev loss: 3.3953701877593994\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   dev_loss █▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ██▇▆▂▂▂▂▂▂▂▂▂▃▃▂▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   dev_loss 3.39677\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 3.28825\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtopic10_word100_BertForMaskedLMCustom_Adam_lr0.01_wd0.0_hiddenlayers1_heads1_hiddendim104_none_one_hot_default_dropout0.0_noise0.0_mask_0.15_correct0.1_random0.1_noMany_freezeUniformAttention_freezeWdecI0\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/yuchenl4/transformer_topic_modeling_learning_dynamics/runs/og1wm7nc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230208_075800-og1wm7nc/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!sh scripts/train_lda_bert.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71oYJb327Hcj"
      },
      "source": [
        "# Plot attention or compute statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjKeS8NG9gYZ",
        "outputId": "54435ce9-dbbb-469d-e659-58d87ab1f9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/bin/bash\n",
            "\n",
            "for hiddenlayers in 1\n",
            "do\n",
            "  for num_heads in 1\n",
            "  do\n",
            "    for lr in 0.01\n",
            "    do\n",
            "      python3 src/inspect_result.py \"config/bert_lda_hiddenlayers\"$hiddenlayers\"_heads\"$num_heads\"_lr\"$lr\"_one_hot.yaml\"\n",
            "    done\n",
            "  done\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "!cat scripts/inspect_result_lda_bert.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcMPD-T9D3TN",
        "outputId": "7baa7d8c-6fb7-42ba-feb5-190468a6a06f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from argparse import ArgumentParser\n",
            "from transformers import BertModel, GPT2Model\n",
            "from seaborn import heatmap\n",
            "import gensim\n",
            "import numpy as np\n",
            "import os\n",
            "import pickle\n",
            "import matplotlib.pyplot as plt\n",
            "import random\n",
            "import torch\n",
            "\n",
            "from run_lm import create_args, init_lm\n",
            "import dataset\n",
            "from topic_model import load_topic_model, get_top_term_topics\n",
            "import utils\n",
            "\n",
            "\n",
            "def cut_sentences_at_length(batch, max_sentence_len):\n",
            "    assert type(batch) is tuple\n",
            "    new_batch0 = batch[0][:, :max_sentence_len].clone().detach()  # observations\n",
            "    new_batch1 = batch[1][:, :max_sentence_len].clone().detach()  # labels\n",
            "    if batch[2] is not None:\n",
            "        new_batch2 = batch[2][:, :max_sentence_len].clone().detach()  # attention_mask_all\n",
            "    else:\n",
            "        new_batch2 = None\n",
            "    new_batch3 = [max_sentence_len]  # sentence length\n",
            "    return new_batch0, new_batch1, new_batch2, new_batch3\n",
            "\n",
            "\n",
            "def prepare_dev_data(dataset, num_sentences_to_plot, max_sentence_len=None):\n",
            "    \"\"\"\n",
            "    Note: one batch contains `arg.training.batch_size` sentences\n",
            "    In this case, `arg.training.batch_size` should be set to 1\n",
            "    Return all the first `num_sentences_to_plot` sentences as a list.\n",
            "    \"\"\"\n",
            "    dev_batches = dataset.get_dev_dataloader()\n",
            "    batches = []\n",
            "    i = 0\n",
            "    for batch in dev_batches:\n",
            "        if max_sentence_len is not None:\n",
            "            batch = cut_sentences_at_length(batch, max_sentence_len)\n",
            "        batches.append(batch)\n",
            "        i += 1\n",
            "        if i == num_sentences_to_plot:\n",
            "            break\n",
            "    return batches\n",
            "\n",
            "\n",
            "def translate_ids_to_tokens(token_ids, token2id):\n",
            "    ids_to_tokens = {token2id[token]: token for token in token2id}\n",
            "    return [ids_to_tokens[token_id] for token_id in token_ids]\n",
            "\n",
            "\n",
            "def get_bert_embeddings(bert, input_ids, token_type_ids):\n",
            "    return bert.embeddings(\n",
            "        input_ids=input_ids,\n",
            "        position_ids=None,\n",
            "        token_type_ids=token_type_ids,\n",
            "        inputs_embeds=None,\n",
            "        past_key_values_length=0,\n",
            "    )\n",
            "\n",
            "\n",
            "def get_attention_bert(model, input_ids, token_type_ids):\n",
            "    \"\"\"\n",
            "    Get the attention weights for ONE sentence\n",
            "    \"\"\"\n",
            "    assert len(input_ids) == 1, 'the input must contain exactly 1 sentence'\n",
            "    all_attention_outputs = model.forward(\n",
            "        input_ids,\n",
            "        token_type_ids=token_type_ids,\n",
            "        output_attentions=True,\n",
            "    ).attentions\n",
            "    all_attention_outputs = np.array([\n",
            "        a[0].cpu().detach().numpy()\n",
            "        for a in all_attention_outputs\n",
            "    ])  # has shape [num_hidden_layers, num_attention_heads, sentence_len, sentence_len]\n",
            "    return all_attention_outputs\n",
            "\n",
            "\n",
            "def plot_attention_bert(lm_model, batches, plot_save_dir):\n",
            "    for sentence_idx, batch in enumerate(batches):\n",
            "        batch_size, seq_length = batch[0].size()\n",
            "        buffered_token_type_ids = lm_model.model.bert.embeddings.token_type_ids[:, :seq_length]\n",
            "        buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
            "        token_type_ids = buffered_token_type_ids_expanded\n",
            "\n",
            "        all_attention_outputs = get_attention_bert(lm_model.model.bert, batch[0], token_type_ids)\n",
            "\n",
            "        tokens = translate_ids_to_tokens(batch[0][0].to('cpu').numpy(), dataset.vocab)\n",
            "        for layer_idx in range(len(all_attention_outputs)):\n",
            "            for head_idx in range(len(all_attention_outputs[0])):\n",
            "                plt.figure(figsize=(15, 12))\n",
            "                ax = plt.axes()\n",
            "                heatmap(all_attention_outputs[layer_idx][head_idx], xticklabels=tokens, yticklabels=tokens)\n",
            "                ax.set_title(f\"sentence{sentence_idx} layer{layer_idx} head{head_idx}\\n{' '.join(tokens)}\")\n",
            "                plt.savefig(\n",
            "                    os.path.join(plot_save_dir, f\"sentence{sentence_idx}_layer{layer_idx}_head{head_idx}.png\"))\n",
            "                plt.show()\n",
            "\n",
            "\n",
            "def get_gpt_embeddings(gpt, input_ids):\n",
            "    return gpt.get_input_embeddings()(input_ids)\n",
            "\n",
            "\n",
            "def get_attention_gpt2(model, input_ids, attention_mask):\n",
            "    \"\"\"\n",
            "    Get the attention weights for ONE sentence\n",
            "    \"\"\"\n",
            "    assert len(input_ids) == 1, 'the input must contain exactly 1 sentence'\n",
            "    all_attention_outputs = np.array([\n",
            "        a[0].cpu().detach().numpy()\n",
            "        for a in model.forward(input_ids, attention_mask=attention_mask, output_attentions=True).attentions\n",
            "    ])  # has shape [num_hidden_layers, num_attention_heads, sentence_len, sentence_len]\n",
            "    return all_attention_outputs\n",
            "\n",
            "\n",
            "def plot_attention_gpt2(lm_model, batches, plot_save_dir):\n",
            "    for sentence_idx, batch in enumerate(batches):\n",
            "        all_attention_outputs = get_attention_gpt2(lm_model.model, batch[0], None)\n",
            "        tokens = translate_ids_to_tokens(batch[0][0].to('cpu').numpy(), dataset.vocab)\n",
            "        for layer_idx in range(len(all_attention_outputs)):\n",
            "            for head_idx in range(len(all_attention_outputs[0])):\n",
            "                plt.figure(figsize=(15, 12))\n",
            "                ax = plt.axes()\n",
            "                heatmap(all_attention_outputs[layer_idx][head_idx], xticklabels=tokens, yticklabels=tokens)\n",
            "                ax.set_title(f\"sentence{sentence_idx} layer{layer_idx} head{head_idx}\\n{' '.join(tokens)}\")\n",
            "                plt.savefig(os.path.join(plot_save_dir, f\"sentence{sentence_idx}_layer{layer_idx}_head{head_idx}.png\"))\n",
            "                plt.show()\n",
            "\n",
            "\n",
            "def compare_topic_and_non_topic_attention(\n",
            "        model,\n",
            "        topic_model,\n",
            "        num_topics_per_word,\n",
            "        data,\n",
            "        max_num_docs,\n",
            "        stop_tokens=set(),\n",
            "        ref_sentence_len=None,\n",
            "        skip_num_docs=0,\n",
            "        filter_ambiguous_words_threshold=0.0,\n",
            "):\n",
            "    num_layers = model.config.num_hidden_layers\n",
            "    num_heads = model.config.num_attention_heads\n",
            "\n",
            "    topic_attn_stat = [\n",
            "        [\n",
            "            {\n",
            "                'same_token_attn': 0.0,\n",
            "                'same_token_cnt': 0,\n",
            "                'topic_attn': 0.0,\n",
            "                'topic_cnt': 0,\n",
            "                'non_topic_attn': 0.0,\n",
            "                'non_topic_cnt': 0,\n",
            "                'empty_topic_attn': 0.0,\n",
            "                'empty_topic_cnt': 0,\n",
            "                'same_token_attn_fraction_raw': [],\n",
            "                'topic_attn_fraction_raw': [],\n",
            "                'non_topic_attn_fraction_raw': [],\n",
            "                'empty_topic_attn_fraction_raw': [],\n",
            "                'max_other_topic_attn_fraction_raw': [],\n",
            "                'same_token_attn_fraction_softmax': [],\n",
            "                'topic_attn_fraction_softmax': [],\n",
            "                'non_topic_attn_fraction_softmax': [],\n",
            "                'empty_topic_attn_fraction_softmax': [],\n",
            "                'max_other_topic_attn_fraction_softmax': [],\n",
            "            }\n",
            "            for head in range(num_heads)\n",
            "        ]\n",
            "        for layer in range(num_layers)\n",
            "    ]\n",
            "    num_topics_in_docs = []\n",
            "    same_token_attn_list = []\n",
            "    topic_attn_list = []\n",
            "    non_topic_attn_list = []\n",
            "    empty_topic_attn_list = []\n",
            "\n",
            "    num_docs = 0\n",
            "    if type(topic_model) is dict:\n",
            "        num_topics = len(set(topic_model.values()))\n",
            "    elif type(topic_model) is gensim.models.ldamodel.LdaModel:\n",
            "        num_topics = topic_model.get_topics().shape[0]\n",
            "    else:\n",
            "        raise NotImplementedError('topic_model should be a gensim.models.ldamodel.LdaModel or dict')\n",
            "\n",
            "    for doc in data:\n",
            "        if skip_num_docs > 0:\n",
            "            skip_num_docs -= 1\n",
            "            continue\n",
            "        if type(doc) is tuple:  # a batch (must contain exactly 1 sentence)\n",
            "            tokens = doc[0][0].to('cpu').numpy()\n",
            "            words = None\n",
            "            batch_size, seq_length = doc[0].size()\n",
            "            buffered_token_type_ids = model.embeddings.token_type_ids[:, :seq_length]\n",
            "            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
            "            token_type_ids = buffered_token_type_ids_expanded\n",
            "            all_attention_outputs = get_attention_bert(model, doc[0], token_type_ids)\n",
            "            if ref_sentence_len is not None:\n",
            "                all_attention_outputs = all_attention_outputs * len(tokens) / ref_sentence_len\n",
            "        else:\n",
            "            print(f\"type(doc): {type(doc)}\")\n",
            "            print(f\"doc: {doc}\")\n",
            "            raise NotImplementedError(\n",
            "                \"doc can either be a dict (an entry of wikidata['train']) or a tuple (an entry of DataLoader)\"\n",
            "            )\n",
            "\n",
            "        n = len(tokens)\n",
            "        # If a word has one topic, include the topic\n",
            "        # If a word has multiple topics, only include the topics that are matched with other words in the doc\n",
            "        matched_topics_of_tokens = [set()] * n\n",
            "        topics = [\n",
            "            get_top_term_topics(topic_model, word, num_topics=num_topics_per_word, filter_ambiguous_words_threshold=filter_ambiguous_words_threshold)\n",
            "            for word in tokens\n",
            "        ]\n",
            "\n",
            "        if all_attention_outputs.shape == (num_layers, num_heads, n, n):\n",
            "            for layer in range(num_layers):\n",
            "                for head in range(num_heads):\n",
            "                    for i in range(n):\n",
            "                        for j in range(n):\n",
            "                            if (tokens[i] not in stop_tokens) and (tokens[j] not in stop_tokens)\\\n",
            "                            and (words is None or tokens[i] in words and tokens[j] in words):  # heuristics: the word is not broken into pieces by tokenization\n",
            "                                if i == j or tokens[i] == tokens[j]:\n",
            "                                    topic_attn_stat[layer][head]['same_token_attn'] += \\\n",
            "                                        all_attention_outputs[layer][head][i][j]\n",
            "                                    topic_attn_stat[layer][head]['same_token_cnt'] += 1\n",
            "                                elif len(topics[i]) == 0 or len(topics[j]) == 0:\n",
            "                                        topic_attn_stat[layer][head]['empty_topic_attn'] += \\\n",
            "                                        all_attention_outputs[layer][head][i][j]\n",
            "                                        topic_attn_stat[layer][head]['empty_topic_cnt'] += 1\n",
            "                                elif len(set(topics[i]).intersection(set(topics[j]))) >= 1:\n",
            "                                    topic_attn_stat[layer][head]['topic_attn'] += all_attention_outputs[layer][head][i][\n",
            "                                        j]\n",
            "                                    topic_attn_stat[layer][head]['topic_cnt'] += 1\n",
            "                                    matched_topics_of_tokens[i] = matched_topics_of_tokens[i].union(\n",
            "                                                                    set(topics[i]).intersection(set(topics[j]))\n",
            "                                                                )\n",
            "                                else:\n",
            "                                    topic_attn_stat[layer][head]['non_topic_attn'] += \\\n",
            "                                    all_attention_outputs[layer][head][i][j]\n",
            "                                    topic_attn_stat[layer][head]['non_topic_cnt'] += 1\n",
            "        else:\n",
            "            print(f\"\\t Shape mismatch: all_attention_outputs.shape=={all_attention_outputs.shape}, n=={n}\")\n",
            "\n",
            "        topics_in_doc = set()\n",
            "        for i in range(n):\n",
            "            if len(matched_topics_of_tokens[i]) == 0:\n",
            "                matched_topics_of_tokens[i] = set(topics[i][:1])\n",
            "            topics_in_doc = topics_in_doc.union(matched_topics_of_tokens[i])\n",
            "        num_topics_in_docs.append(len(topics_in_doc))\n",
            "\n",
            "        num_docs += 1\n",
            "        if num_docs % 100 == 0:\n",
            "            print(f\"Processed {num_docs} documents.\")\n",
            "        if num_docs >= max_num_docs:\n",
            "            break\n",
            "\n",
            "    return topic_attn_stat, num_topics_in_docs, same_token_attn_list, topic_attn_list, non_topic_attn_list, empty_topic_attn_list\n",
            "\n",
            "\n",
            "def save_topic_and_non_topic_attention_results(\n",
            "        num_topics_per_word,\n",
            "        inspect_results_dir,\n",
            "        topic_attn_stat,\n",
            "        topic_attn_stat_fn,\n",
            "):\n",
            "    text_to_save = f\"Computing statistics for num_topics_per_word={num_topics_per_word}\\n\\n\"\n",
            "    print(text_to_save)\n",
            "\n",
            "    for layer in range(len(topic_attn_stat)):\n",
            "        for head in range(len(topic_attn_stat[0])):\n",
            "            if topic_attn_stat[layer][head]['same_token_cnt'] == 0:\n",
            "                print(f\"Warning: unexpected: topic_attn_stat[{layer}][{head}]['same_token_cnt'] == 0\")\n",
            "                topic_attn_stat[layer][head]['same_token_avg_attn'] = 0.0\n",
            "            else:\n",
            "                topic_attn_stat[layer][head]['same_token_avg_attn'] = \\\n",
            "                    topic_attn_stat[layer][head]['same_token_attn'] / topic_attn_stat[layer][head]['same_token_cnt']\n",
            "            if topic_attn_stat[layer][head]['topic_cnt'] == 0:\n",
            "                topic_attn_stat[layer][head]['topic_avg_attn'] = 0.0\n",
            "            else:\n",
            "                topic_attn_stat[layer][head]['topic_avg_attn'] = \\\n",
            "                    topic_attn_stat[layer][head]['topic_attn'] / topic_attn_stat[layer][head]['topic_cnt']\n",
            "            if topic_attn_stat[layer][head]['non_topic_cnt'] == 0:\n",
            "                topic_attn_stat[layer][head]['non_topic_avg_attn'] = 0.0\n",
            "            else:\n",
            "                topic_attn_stat[layer][head]['non_topic_avg_attn'] = \\\n",
            "                    topic_attn_stat[layer][head]['non_topic_attn'] / topic_attn_stat[layer][head]['non_topic_cnt']\n",
            "            if topic_attn_stat[layer][head]['empty_topic_cnt'] == 0:\n",
            "                topic_attn_stat[layer][head]['empty_topic_avg_attn'] = 0.0\n",
            "            else:\n",
            "                topic_attn_stat[layer][head]['empty_topic_avg_attn'] = \\\n",
            "                    topic_attn_stat[layer][head]['empty_topic_attn'] / topic_attn_stat[layer][head]['empty_topic_cnt']\n",
            "            log_str = (\n",
            "                f\"layer {layer} head {head} \\\n",
            "                same_token_avg_attn: {topic_attn_stat[layer][head]['same_token_avg_attn']}, \\\n",
            "                same_token_cnt: {topic_attn_stat[layer][head]['same_token_cnt']}, \\\n",
            "                topic_avg_attn: {topic_attn_stat[layer][head]['topic_avg_attn']}, \\\n",
            "                topic_cnt: {topic_attn_stat[layer][head]['topic_cnt']}, \\\n",
            "                non_topic_avg_attn: {topic_attn_stat[layer][head]['non_topic_avg_attn']}, \\\n",
            "                non_topic_cnt: {topic_attn_stat[layer][head]['non_topic_cnt']}, \\\n",
            "                empty_topic_avg_attn: {topic_attn_stat[layer][head]['empty_topic_avg_attn']}, \\\n",
            "                empty_topic_cnt: {topic_attn_stat[layer][head]['empty_topic_cnt']}, \\\n",
            "                (topic_avg_attn - non_topic_avg_attn): {topic_attn_stat[layer][head]['topic_avg_attn'] - topic_attn_stat[layer][head]['non_topic_avg_attn']}\"\n",
            "            )\n",
            "            print(log_str)\n",
            "            text_to_save += log_str + '\\n\\n'\n",
            "\n",
            "            log_str = 'Plotting fractions of same vs. different topic attention'\n",
            "            print(log_str)\n",
            "            text_to_save += log_str + '\\n\\n'\n",
            "\n",
            "    with open(topic_attn_stat_fn, 'wb') as f:\n",
            "        pickle.dump(topic_attn_stat, f)\n",
            "    with open(os.path.join(inspect_results_dir, f\"topic_attn_stat_top{num_topics_per_word}.txt\"), 'wt') as f:\n",
            "        f.write(text_to_save)\n",
            "\n",
            "\n",
            "def check_bert_embedding_dot_product_topic(\n",
            "        model,\n",
            "        tokenizer,\n",
            "        topic_model,\n",
            "        num_topics_per_word,\n",
            "        inspect_results_dir,\n",
            "        stop_tokens=set(),\n",
            "        filter_ambiguous_words_threshold=0.0,\n",
            "):\n",
            "    text_to_save = f\"Computing embedding dot product for num_topics_per_word={num_topics_per_word}\\n\\n\"\n",
            "    print(text_to_save)\n",
            "\n",
            "    if type(model) in [BertModel]:\n",
            "        emb = model.embeddings.word_embeddings.weight.data.cpu().numpy()\n",
            "    elif type(model) in [GPT2Model]:\n",
            "        emb = model.wte.weight.data.cpu().numpy()\n",
            "    else:\n",
            "        raise NotImplementedError('model type not supported')\n",
            "    tokens = tokenizer.get_vocab().keys()\n",
            "    tokens = [token for token in tokens if token[0] != '[']  # filter special characters like '[PAD]', '[unused0]'\n",
            "    # First 1000 tokens are mostly unused placeholders.\n",
            "    n = len(tokens)\n",
            "    topics = [\n",
            "        get_top_term_topics(topic_model, word, num_topics=num_topics_per_word, filter_ambiguous_words_threshold=filter_ambiguous_words_threshold)\n",
            "        for word in tokens\n",
            "    ]\n",
            "\n",
            "    same_token_dot_list = []\n",
            "    topic_dot_list = []\n",
            "    non_topic_dot_list = []\n",
            "    empty_topic_dot_list = []\n",
            "\n",
            "    cnt = 0\n",
            "    for i in random.sample(range(n), 10000):\n",
            "        if tokens[i] not in stop_tokens:\n",
            "            same_token_dot_list.append(emb[i] @ emb[i])\n",
            "        for j in random.sample(range(n), 10000):\n",
            "            if (tokens[i] not in stop_tokens) and (tokens[j] not in stop_tokens):\n",
            "                if len(topics[i]) == 0 or len(topics[j]) == 0:\n",
            "                    empty_topic_dot_list.append(emb[i] @ emb[j])\n",
            "                elif len(set(topics[i]).intersection(set(topics[j]))) >= 1:\n",
            "                    topic_dot_list.append(emb[i] @ emb[j])\n",
            "                else:\n",
            "                    non_topic_dot_list.append(emb[i] @ emb[j])\n",
            "        cnt += 1\n",
            "        if cnt % 1000 == 0:\n",
            "            print(f\"Processed {cnt / 100}% of the planned 1e8 embedding dot products.\")\n",
            "\n",
            "    log_str = (\n",
            "        f\"same_token_avg_emb_dot: {np.mean(same_token_dot_list)}, std {np.std(same_token_dot_list)}\\n \\\n",
            "        same_topic_diff_token_avg_emb_dot: {np.mean(topic_dot_list)}, std {np.std(topic_dot_list)}\\n \\\n",
            "        non_topic_avg_emb_dot: {np.mean(non_topic_dot_list)}, std {np.std(non_topic_dot_list)}\\n \\\n",
            "        empty_topic_avg_emb_dot: {np.mean(empty_topic_dot_list)} std {np.std(empty_topic_dot_list)}\\n \\\n",
            "        (same_topic_diff_token - diff_topic): {np.mean(topic_dot_list) - np.mean(non_topic_dot_list)}\\n\"\n",
            "    )\n",
            "    print(log_str)\n",
            "    text_to_save += log_str + '\\n\\n'\n",
            "\n",
            "    with open(os.path.join(inspect_results_dir, f\"emb_dot_top{num_topics_per_word}.txt\"), 'wt') as f:\n",
            "        f.write(text_to_save)\n",
            "\n",
            "\n",
            "def check_bert_embedding_weights(lm_model, inspect_results_dir):\n",
            "    emb = lm_model.model.bert.embeddings.word_embeddings.weight.data.cpu().numpy()\n",
            "\n",
            "    fn = os.path.join(inspect_results_dir, 'embedding.txt')\n",
            "    with open(fn, 'wt') as f:\n",
            "        f.write('emb.shape\\n')\n",
            "        f.write(str(emb.shape))\n",
            "        f.write('\\n\\n')\n",
            "\n",
            "        f.write('emb\\n')\n",
            "        f.write(str(emb))\n",
            "        f.write('\\n\\n')\n",
            "\n",
            "        dot_prod = emb @ emb.T\n",
            "        f.write('dot_prod\\n')\n",
            "        f.write(str(dot_prod))\n",
            "        f.write('\\n\\n')\n",
            "\n",
            "    plt.figure(figsize=(15, 12))\n",
            "    ax = plt.axes()\n",
            "    heatmap(dot_prod)\n",
            "    ax.set_title(f\"embedding_dot_prod\")\n",
            "    plt.savefig(os.path.join(inspect_results_dir, f\"embedding_dot_prod.png\"))\n",
            "    plt.show()\n",
            "\n",
            "\n",
            "def check_bert_attention_weights(\n",
            "        lm_model,\n",
            "        inspect_results_dir,\n",
            "        num_topics=None,\n",
            "):\n",
            "    bert = lm_model.model.bert\n",
            "    num_layers = bert.config.num_hidden_layers\n",
            "\n",
            "    fn = os.path.join(inspect_results_dir, 'attention.txt')\n",
            "    with open(fn, 'wt') as f:\n",
            "        for i in range(num_layers):\n",
            "            attn_weights = bert.encoder.layer[0].attention.self\n",
            "\n",
            "            # Key\n",
            "            Wk = attn_weights.key.weight.detach().cpu().numpy()\n",
            "            f.write(f\"layer{i}_key\\n\")\n",
            "            f.write(str(Wk))\n",
            "            f.write('\\n\\n')\n",
            "            plt.figure(figsize=(15, 12))\n",
            "            ax = plt.axes()\n",
            "            heatmap(Wk)\n",
            "            ax.set_title(f\"layer{i}_key\")\n",
            "            plt.savefig(os.path.join(inspect_results_dir, f\"layer{i}_key.png\"))\n",
            "            plt.show()\n",
            "\n",
            "            # Query\n",
            "            Wq = attn_weights.query.weight.detach().cpu().numpy()\n",
            "            f.write(f\"layer{i}_query\\n\")\n",
            "            f.write(str(Wq))\n",
            "            f.write('\\n\\n')\n",
            "            plt.figure(figsize=(15, 12))\n",
            "            ax = plt.axes()\n",
            "            heatmap(Wq)\n",
            "            ax.set_title(f\"layer{i}_query\")\n",
            "            plt.savefig(os.path.join(inspect_results_dir, f\"layer{i}_query.png\"))\n",
            "            plt.show()\n",
            "\n",
            "            # Value\n",
            "            Wv = attn_weights.value.weight.detach().cpu().numpy()\n",
            "            f.write(f\"layer{i}_value\\n\")\n",
            "            f.write(str(Wv))\n",
            "            f.write('\\n\\n')\n",
            "            plt.figure(figsize=(15, 12))\n",
            "            ax = plt.axes()\n",
            "            heatmap(Wv)\n",
            "            ax.set_title(f\"layer{i}_value\")\n",
            "            plt.savefig(os.path.join(inspect_results_dir, f\"layer{i}_value.png\"))\n",
            "            plt.show()\n",
            "\n",
            "            # Column-wise dot product\n",
            "            (m, n) = Wv.shape\n",
            "            assert m == n\n",
            "            Wv_column_dot_products = [\n",
            "                [Wv[:, k].T @ Wv[:, j] for j in range(n)]\n",
            "                for k in range(n)\n",
            "            ]\n",
            "            f.write(f\"layer{i}_value column-wise dot product\\n\")\n",
            "            f.write(str(Wv_column_dot_products))\n",
            "            f.write('\\n\\n')\n",
            "            plt.figure(figsize=(15, 12))\n",
            "            ax = plt.axes()\n",
            "            heatmap(Wv_column_dot_products)\n",
            "            ax.set_title(f\"layer{i}_value column-wise dot product\")\n",
            "            plt.savefig(os.path.join(inspect_results_dir, f\"layer{i}_value_column_dot.png\"))\n",
            "            plt.show()\n",
            "\n",
            "            # Wk.T @ Wq\n",
            "            WkTWq = Wk.T @ Wq\n",
            "            f.write(f\"layer{i} Wk.T @ Wq\\n\")\n",
            "            f.write(str(WkTWq))\n",
            "            f.write('\\n\\n')\n",
            "            plt.figure(figsize=(15, 12))\n",
            "            ax = plt.axes()\n",
            "            heatmap(WkTWq)\n",
            "            ax.set_title(f\"layer{i} Wk.T @ Wq\")\n",
            "            plt.savefig(os.path.join(inspect_results_dir, f\"layer{i}_WkTWq.png\"))\n",
            "            plt.show()\n",
            "\n",
            "\n",
            "def get_hidden_representations(model, input_ids, token_type_ids):\n",
            "    \"\"\"\n",
            "    Get the hidden representations (including embedding layer output and final output) for ONE sentence\n",
            "\n",
            "    Return a numpy array of shape [num_hidden_layers, sentence_len, hidden_dim]\n",
            "    \"\"\"\n",
            "    assert len(input_ids) == 1, 'the input must contain exactly 1 sentence'\n",
            "    hidden_states = model.forward(\n",
            "        input_ids,\n",
            "        token_type_ids=token_type_ids,\n",
            "        output_attentions=False,\n",
            "        output_hidden_states=True,\n",
            "    ).hidden_states  # tuple of tensors, shape [num_hidden_layers, 1, sentence_len, hidden_dim]\n",
            "    return np.array([\n",
            "        hidden_state[0].detach().numpy()\n",
            "        for hidden_state in hidden_states\n",
            "    ])\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    # For synthetic dataset\n",
            "\n",
            "    # Parse args\n",
            "    argp = ArgumentParser()\n",
            "    argp.add_argument('config')\n",
            "    args = argp.parse_args()\n",
            "    args = create_args(args.config)\n",
            "    # Only one sentence in a batch, so that displayed sentence length is different for different sentences\n",
            "    args['training']['batch_size'] = 1\n",
            "    # Do not mask any token during evaluation\n",
            "    args['training']['mask_prob'] = 0.0\n",
            "\n",
            "    dataset = dataset.Dataset(args)\n",
            "\n",
            "    name_base = args['name']\n",
            "    for experiment_index in range(args['experiment']['repeat']):\n",
            "        args['name'] = name_base + str(experiment_index)\n",
            "\n",
            "        lm_model = init_lm(args)\n",
            "\n",
            "        if not args['reporting']['random']:\n",
            "            lm_params_path = utils.get_lm_path_of_args(args)\n",
            "            if not os.path.exists(lm_params_path):\n",
            "                print(f\"Warning: trained model does not exist: {lm_params_path}\")\n",
            "                continue\n",
            "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Determine whether CUDA is available\n",
            "            lm_model.load_state_dict(\n",
            "                torch.load(lm_params_path, map_location=device),\n",
            "            )\n",
            "\n",
            "        if args['lm']['freeze_uniform_attention']:\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.key.weight.data = torch.zeros(\n",
            "                (args['lm']['hidden_dim'], args['lm']['hidden_dim']), device=args['device'])\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.key.weight.requires_grad = False\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.key.bias.data = torch.zeros(\n",
            "                (args['lm']['hidden_dim'],), device=args['device'])\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.key.bias.requires_grad = False\n",
            "\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.query.weight.data = torch.zeros(\n",
            "                (args['lm']['hidden_dim'], args['lm']['hidden_dim']), device=args['device'])\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.query.weight.requires_grad = False\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.query.bias.data = torch.zeros(\n",
            "                (args['lm']['hidden_dim'],), device=args['device'])\n",
            "            lm_model.model.bert.encoder.layer[0].attention.self.query.bias.requires_grad = False\n",
            "\n",
            "        lm_model.eval()\n",
            "\n",
            "        inspect_results_dir = os.path.join(args['reporting']['inspect_results_dir'], args['name'])\n",
            "        os.makedirs(inspect_results_dir, exist_ok=True)\n",
            "\n",
            "        # Plot attention\n",
            "        num_sentences_to_plot = args['reporting']['num_sentences_to_plot']\n",
            "        batches = prepare_dev_data(dataset, num_sentences_to_plot)\n",
            "\n",
            "        plot_save_dir = os.path.join(args['reporting']['plot_attention_dir'], args['name'])\n",
            "        os.makedirs(plot_save_dir, exist_ok=True)\n",
            "\n",
            "        if args['lm']['lm_type'] in {'BertForMaskedLM', 'BertForMaskedLMCustom'}:\n",
            "            plot_attention_bert(lm_model, batches, plot_save_dir)\n",
            "        elif args['lm']['lm_type'] == 'GPT2LMHeadModel':\n",
            "            plot_attention_gpt2(lm_model, batches, plot_save_dir)\n",
            "        else:\n",
            "            raise NotImplementedError('Model not supported.')\n",
            "\n",
            "        # Compute topic-wise statistics\n",
            "        if args['language']['name'] == 'lda':\n",
            "            max_num_docs = 500\n",
            "            batches = prepare_dev_data(dataset, max_num_docs, max_sentence_len=100)\n",
            "            topic_model = load_topic_model(\"./trained_models/topic10_word100.pkl\")\n",
            "\n",
            "            for num_topics_per_word in [1]:  # should be > 1 if different topics have overlapping words\n",
            "                topic_attn_stat, num_topics_in_docs, same_token_attn_list, topic_attn_list, non_topic_attn_list, empty_topic_attn_list = \\\n",
            "                                                        compare_topic_and_non_topic_attention(\n",
            "                                                            lm_model.model.bert,\n",
            "                                                            topic_model,\n",
            "                                                            num_topics_per_word,\n",
            "                                                            batches,\n",
            "                                                            max_num_docs,\n",
            "                                                            stop_tokens={'PAD', 'MASK', 'START', 'END'},\n",
            "                                                            ref_sentence_len=100,\n",
            "                                                            skip_num_docs=0,\n",
            "                                                        )\n",
            "\n",
            "                with open(f\"./trained_models/{args['name']}_num_topics_in_docs_top{num_topics_per_word}.pkl\", 'wb') as f:\n",
            "                    pickle.dump(num_topics_in_docs, f)\n",
            "\n",
            "                save_topic_and_non_topic_attention_results(\n",
            "                    num_topics_per_word,\n",
            "                    inspect_results_dir,\n",
            "                    topic_attn_stat,\n",
            "                    f\"./trained_models/{args['name']}_topic_attn_stat_top{num_topics_per_word}.pkl\",\n",
            "                )\n",
            "\n",
            "        # Check weights\n",
            "        # TODO: add support for GPT\n",
            "        if args['lm']['lm_type'] in {'BertForMaskedLM', 'BertForMaskedLMCustom'}:\n",
            "            check_bert_embedding_weights(lm_model, inspect_results_dir)\n",
            "            check_bert_attention_weights(\n",
            "                lm_model,\n",
            "                inspect_results_dir,\n",
            "                args['language']['num_topics'] if 'num_topics' in args['language'] else None,\n",
            "            )\n",
            "\n",
            "        # # Store the BERT models\n",
            "        # bert_dir = os.path.join(args['reporting']['reporting_loc'], 'bert/')\n",
            "        # os.makedirs(bert_dir, exist_ok=True)\n",
            "        # with open(bert_dir + f\"{args['name']}.pkl\", 'wb') as f:\n",
            "        #     pickle.dump(\n",
            "        #         lm_model.model.bert,\n",
            "        #         f,\n",
            "        #     )\n",
            "\n",
            "        # Store contextual representations\n",
            "        # TODO: add support for GPT\n",
            "        if args['lm']['lm_type'] in {'BertForMaskedLM', 'BertForMaskedLMCustom'}:\n",
            "            max_num_docs = 1000\n",
            "            batches = prepare_dev_data(dataset, max_num_docs, max_sentence_len=512)\n",
            "            all_hidden_states = []\n",
            "            all_outputs = []\n",
            "\n",
            "            for batch in batches:\n",
            "                tokens = batch[0][0].to('cpu').numpy()\n",
            "                words = None\n",
            "                batch_size, seq_length = batch[0].size()\n",
            "                buffered_token_type_ids = lm_model.model.bert.embeddings.token_type_ids[:, :seq_length]\n",
            "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
            "                token_type_ids = buffered_token_type_ids_expanded\n",
            "\n",
            "                all_hidden_states.append(\n",
            "                    get_hidden_representations(\n",
            "                        lm_model.model.bert,\n",
            "                        batch[0],\n",
            "                        token_type_ids,\n",
            "                    )\n",
            "                )\n",
            "\n",
            "            rep_fn = os.path.join(inspect_results_dir, 'rep.pkl')\n",
            "            with open(rep_fn, 'wb') as f:\n",
            "                pickle.dump(all_hidden_states, f)\n",
            "\n",
            "            print(f\"Saved hidden representations at {rep_fn}\")\n"
          ]
        }
      ],
      "source": [
        "!cat src/inspect_result.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-Lj05qr50Y-",
        "outputId": "a23c41b3-99ee-458d-a229-9419622ad4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/research/project/hierarchical_compositionality/codes/grammar_compositionality_202302/src/run_lm.py:23: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  args = yaml.load(open(config_file))\n",
            "Getting dataset from data/topic10_word100_long.train\n",
            "[computing labels]: 100% 10000/10000 [00:00<00:00, 20244.14it/s]\n",
            "Getting dataset from data/topic10_word100_long.dev\n",
            "[computing labels]: 100% 10000/10000 [00:00<00:00, 20693.30it/s]\n",
            "Getting dataset from data/topic10_word100_long.test\n",
            "[computing labels]: 100% 10000/10000 [00:00<00:00, 13213.52it/s]\n",
            "config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"attn_output_fc\": false,\n",
            "  \"bert_head_transform\": false,\n",
            "  \"bert_intermediate\": false,\n",
            "  \"bert_output\": false,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"freeze_block_value_matrix\": false,\n",
            "  \"freeze_decoder_to_I\": true,\n",
            "  \"freeze_id_value_matrix\": false,\n",
            "  \"freeze_uniform_attention\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 104,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 104,\n",
            "  \"layer_norm\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 6000,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"residual\": false,\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 104,\n",
            "  \"zero_init_attn\": false,\n",
            "  \"zero_init_emb_dec\": false,\n",
            "  \"zero_init_noise\": 0.0\n",
            "}\n",
            "\n",
            "model BertForMaskedLMCustom(\n",
            "  (bert): BertModelCustom(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(104, 104, padding_idx=0)\n",
            "      (position_embeddings): Embedding(6000, 104)\n",
            "      (token_type_embeddings): Embedding(2, 104)\n",
            "      (LayerNorm): LayerNorm((104,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoderCustom(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayerCustom(\n",
            "          (attention): BertAttentionCustom(\n",
            "            (self): BertSelfAttentionCustom(\n",
            "              (query): Linear(in_features=104, out_features=104, bias=True)\n",
            "              (key): Linear(in_features=104, out_features=104, bias=True)\n",
            "              (value): Linear(in_features=104, out_features=104, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutputCustom(\n",
            "              (LayerNorm): Identity()\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): None\n",
            "          (output): None\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): None\n",
            "  )\n",
            "  (cls): BertOnlyMLMHeadCustom(\n",
            "    (predictions): BertLMPredictionHeadCustom(\n",
            "      (transform): None\n",
            "      (decoder): Linear(in_features=104, out_features=104, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Constructing a BertForMaskedLMCustom pytorch model w hidden size 104, layers 1, dropout 0.0\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Processed 100 documents.\n",
            "Processed 200 documents.\n",
            "Processed 300 documents.\n",
            "Processed 400 documents.\n",
            "Processed 500 documents.\n",
            "Computing statistics for num_topics_per_word=1\n",
            "\n",
            "\n",
            "layer 0 head 0                 same_token_avg_attn: 0.009999999776482582,                 same_token_cnt: 319724,                 topic_avg_attn: 0.009999999776482582,                 topic_cnt: 1214976,                 non_topic_avg_attn: 0.009999999776482582,                 non_topic_cnt: 3132390,                 empty_topic_avg_attn: 0.009999999776482582,                 empty_topic_cnt: 332910,                 (topic_avg_attn - non_topic_avg_attn): 0.0\n",
            "Plotting fractions of same vs. different topic attention\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Figure(1500x1200)\n",
            "Saved hidden representations at ./inspect_results/lda_bert_simplified_one_hot/topic10_word100_BertForMaskedLMCustom_Adam_lr0.01_wd0.0_hiddenlayers1_heads1_hiddendim104_none_one_hot_default_dropout0.0_noise0.0_mask_0.15_correct0.1_random0.1_noMany_freezeUniformAttention_freezeWdecI0/rep.pkl\n"
          ]
        }
      ],
      "source": [
        "!sh scripts/inspect_result_lda_bert.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDg2yq8jOw3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-6rmiHHwU-3b"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}