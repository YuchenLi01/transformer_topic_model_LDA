{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS4CbKpkxmnu"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3EPhRgYqiTv"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put this notebook in the folder `transformer_topic_model_LDA/` (root of this repo)"
      ],
      "metadata": {
        "id": "L0uLlElLVC2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LA2HNyCj22e"
      },
      "outputs": [],
      "source": [
        "!pip install -U wandb\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGL_FPpUj4Qv"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myL5N63Mj6H8"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data setting"
      ],
      "metadata": {
        "id": "2776BtKhuoa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qElSHE6SVBZd",
        "outputId": "ecd7f454-5963-41bf-f12c-665bd74cd2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic_term_matrix\n",
            "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n",
            "  0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1\n",
            "  0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1\n",
            "  0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1\n",
            "  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "num_words = 100\n",
        "num_topics = 10\n",
        "num_words_per_topic = num_words // num_topics\n",
        "topic_model = {}\n",
        "\n",
        "topic_term_matrix = np.zeros((num_topics, num_words))\n",
        "for i in range(num_topics):\n",
        "    words_of_topic_i = range(num_words_per_topic * i, num_words_per_topic * (i+1))\n",
        "    for word in words_of_topic_i:\n",
        "        topic_term_matrix[i][word] = num_topics / num_words\n",
        "        topic_model[word] = i\n",
        "\n",
        "print('topic_term_matrix')\n",
        "print(topic_term_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwxgf49grXzW"
      },
      "outputs": [],
      "source": [
        "num_train_sentences = 10000\n",
        "num_dev_sentences = 10000\n",
        "num_test_sentences = 10000\n",
        "sentence_len_min = 100\n",
        "sentence_len_max = 150\n",
        "alpha = [0.1] * num_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwRphJJ3S6I2"
      },
      "source": [
        "## Only run if need to re-generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W_TmtAaIe_g"
      },
      "outputs": [],
      "source": [
        "with open(f\"./trained_models/topic{num_topics}_word{num_words}.pkl\", 'wb') as f:\n",
        "    pickle.dump(topic_model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3AqFJ5eVYqH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from scipy.stats import dirichlet\n",
        "\n",
        "\n",
        "def write_lda_data(fn, num_sentences):\n",
        "    with open(fn, 'wt') as f:\n",
        "        for i in range(num_sentences):\n",
        "            sentence_len = random.randint(sentence_len_min, sentence_len_max)\n",
        "            topic_distr = dirichlet.rvs(alpha, size=None)\n",
        "            for _ in range(sentence_len):\n",
        "                topic = np.random.choice(range(num_topics), p=topic_distr)\n",
        "                word = np.random.choice(range(num_words), p=topic_term_matrix[topic])\n",
        "                f.write(f\"{word} \")\n",
        "            f.write(f\"END\\n\")\n",
        "\n",
        "write_lda_data(f\"data/topic{num_topics}_word{num_words}_long.train\", num_train_sentences)\n",
        "write_lda_data(f\"data/topic{num_topics}_word{num_words}_long.dev\", num_dev_sentences)\n",
        "write_lda_data(f\"data/topic{num_topics}_word{num_words}_long.test\", num_test_sentences)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate config"
      ],
      "metadata": {
        "id": "XQBRx6OlxvK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJEXOKOa6tR"
      },
      "outputs": [],
      "source": [
        "def gen_vocab_str(num_words):\n",
        "    vocab = dict(zip(['PAD', 'MASK', 'START', 'END'] + list(range(num_words)), range(num_words+4)))\n",
        "\n",
        "    vocab_str = 'vocab:'\n",
        "    for token in vocab:\n",
        "        vocab_str += f\"\\n{' ' * 26}'{token}': {vocab[token]}\"\n",
        "    return vocab_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wl0-uJ-kiXE",
        "outputId": "15b70a65-c531-4c68-b2ce-8a8b84ad24f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab:\n",
            "                          'PAD': 0\n",
            "                          'MASK': 1\n",
            "                          'START': 2\n",
            "                          'END': 3\n",
            "                          '0': 4\n",
            "                          '1': 5\n",
            "                          '2': 6\n",
            "                          '3': 7\n",
            "                          '4': 8\n",
            "                          '5': 9\n",
            "                          '6': 10\n",
            "                          '7': 11\n",
            "                          '8': 12\n",
            "                          '9': 13\n",
            "                          '10': 14\n",
            "                          '11': 15\n",
            "                          '12': 16\n",
            "                          '13': 17\n",
            "                          '14': 18\n",
            "                          '15': 19\n",
            "                          '16': 20\n",
            "                          '17': 21\n",
            "                          '18': 22\n",
            "                          '19': 23\n",
            "                          '20': 24\n",
            "                          '21': 25\n",
            "                          '22': 26\n",
            "                          '23': 27\n",
            "                          '24': 28\n",
            "                          '25': 29\n",
            "                          '26': 30\n",
            "                          '27': 31\n",
            "                          '28': 32\n",
            "                          '29': 33\n",
            "                          '30': 34\n",
            "                          '31': 35\n",
            "                          '32': 36\n",
            "                          '33': 37\n",
            "                          '34': 38\n",
            "                          '35': 39\n",
            "                          '36': 40\n",
            "                          '37': 41\n",
            "                          '38': 42\n",
            "                          '39': 43\n",
            "                          '40': 44\n",
            "                          '41': 45\n",
            "                          '42': 46\n",
            "                          '43': 47\n",
            "                          '44': 48\n",
            "                          '45': 49\n",
            "                          '46': 50\n",
            "                          '47': 51\n",
            "                          '48': 52\n",
            "                          '49': 53\n",
            "                          '50': 54\n",
            "                          '51': 55\n",
            "                          '52': 56\n",
            "                          '53': 57\n",
            "                          '54': 58\n",
            "                          '55': 59\n",
            "                          '56': 60\n",
            "                          '57': 61\n",
            "                          '58': 62\n",
            "                          '59': 63\n",
            "                          '60': 64\n",
            "                          '61': 65\n",
            "                          '62': 66\n",
            "                          '63': 67\n",
            "                          '64': 68\n",
            "                          '65': 69\n",
            "                          '66': 70\n",
            "                          '67': 71\n",
            "                          '68': 72\n",
            "                          '69': 73\n",
            "                          '70': 74\n",
            "                          '71': 75\n",
            "                          '72': 76\n",
            "                          '73': 77\n",
            "                          '74': 78\n",
            "                          '75': 79\n",
            "                          '76': 80\n",
            "                          '77': 81\n",
            "                          '78': 82\n",
            "                          '79': 83\n",
            "                          '80': 84\n",
            "                          '81': 85\n",
            "                          '82': 86\n",
            "                          '83': 87\n",
            "                          '84': 88\n",
            "                          '85': 89\n",
            "                          '86': 90\n",
            "                          '87': 91\n",
            "                          '88': 92\n",
            "                          '89': 93\n",
            "                          '90': 94\n",
            "                          '91': 95\n",
            "                          '92': 96\n",
            "                          '93': 97\n",
            "                          '94': 98\n",
            "                          '95': 99\n",
            "                          '96': 100\n",
            "                          '97': 101\n",
            "                          '98': 102\n",
            "                          '99': 103\n"
          ]
        }
      ],
      "source": [
        "print(gen_vocab_str(num_words))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hiddenlayers in [1]:\n",
        "  for num_heads in [1]:\n",
        "    for optimizer in ['Adam']:  # ['Adam', 'SGD']\n",
        "        for lr in [0.01]:\n",
        "            config_text = f\"\"\"\n",
        "            corpus:\n",
        "                train_corpus_loc: data/topic10_word100_long.train\n",
        "                dev_corpus_loc: data/topic10_word100_long.dev\n",
        "                test_corpus_loc:  data/topic10_word100_long.test\n",
        "            language:\n",
        "                name: lda\n",
        "                num_topics: {num_topics}\n",
        "                num_words: {num_words}\n",
        "                {gen_vocab_str(num_words)}\n",
        "                dev_sample_count:  {num_dev_sentences}\n",
        "                test_sample_count: {num_test_sentences}\n",
        "                train_sample_count: {num_train_sentences}\n",
        "            lm:\n",
        "                embedding_dim: {(num_words + 4)}\n",
        "                hidden_dim: {(num_words + 4)}\n",
        "                lm_type: BertForMaskedLMCustom\n",
        "                residual: False  # TODO whether the self attention has residual connections\n",
        "                attn_output_fc: False  # TODO whether the self attention output has a fully connected layer\n",
        "                bert_intermediate: False  # TODO whether the BertLayer has a BertIntermediate (FC) sub-layer\n",
        "                bert_output: False  # TODO whether the BertLayer has a BertOutput (FC with residual) sub-layer\n",
        "                bert_head_transform: False  # whether the BertLMPredictionHead has a transform (FC) sub-layer\n",
        "                layer_norm: False  # whether the model has LayerNorm\n",
        "                num_layers: {hiddenlayers}\n",
        "                save_path: lm.params\n",
        "                num_heads: {num_heads}\n",
        "                embedding_type: none\n",
        "                token_embedding_type: one_hot  # trained or one_hot\n",
        "                freeze_uniform_attention: True  # TODO freeze W^K and W^Q to 0\n",
        "                freeze_id_value_matrix: False  # TODO freeze W^V to I\n",
        "                freeze_block_value_matrix: False  # TODO\n",
        "                freeze_decoder_to_I: True\n",
        "                no_softmax: False  # remove the final softmax layer and change the loss to MSELoss\n",
        "            reporting:\n",
        "                reporting_loc: ./trained_models/lda_bert_simplified_one_hot/  # TODO\n",
        "                reporting_methods:\n",
        "                - constraints\n",
        "                plot_attention_dir: ./plot_attention/lda_bert_simplified_one_hot/  # TODO\n",
        "                inspect_results_dir: ./inspect_results/lda_bert_simplified_one_hot/  # TODO\n",
        "                num_sentences_to_plot: 5\n",
        "                random: False  # TODO\n",
        "                log_all_steps_until: 0  # log all the first several steps to wandb\n",
        "            training:\n",
        "                batch_size: 40\n",
        "                dropout: 0.0\n",
        "                optimizer: {optimizer}  # Adam or SGD\n",
        "                learning_rate: {lr}\n",
        "                weight_decay: 0.0\n",
        "                max_epochs: 20  # LIKELY TOO LOW, JUST A DEMO\n",
        "                seed: 0\n",
        "                objective: default  # default or contrastive or multi\n",
        "                mask_prob: 0.15  # Should almost always be 0.0 for GPT\n",
        "                mask_correct_prob: 0.1  # the proportion of \"masked\" tokens that show the correct token\n",
        "                mask_random_prob: 0.1  # the proportion of \"masked\" tokens that show a random token\n",
        "                zero_init_attn: False  # init W^K, W^Q, W^V to near 0\n",
        "                zero_init_emb_dec: False  # init embedding and decoder to near 0\n",
        "                zero_init_noise: 0.0  # noise for `near 0`\n",
        "            experiment:\n",
        "                repeat: 1  # number of times to re-train the model\n",
        "            \"\"\"\n",
        "            with open(f\"config/bert_lda_hiddenlayers{hiddenlayers}_heads{num_heads}_lr{lr}_one_hot.yaml\", 'wt') as f:\n",
        "                f.write(config_text)"
      ],
      "metadata": {
        "id": "KD3OgWFExxGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3QMT19u4vig"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "518c-NjG87Bd",
        "outputId": "fd512da6-5fe2-4447-c7e4-211f2e383b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/bin/bash\n",
            "\n",
            "for hiddenlayers in 1\n",
            "do\n",
            "  for num_heads in 1\n",
            "  do\n",
            "    for lr in 0.01\n",
            "    do\n",
            "      python3 src/run_lm.py \"config/bert_lda_hiddenlayers\"$hiddenlayers\"_heads\"$num_heads\"_lr\"$lr\"_one_hot.yaml\"\n",
            "    done\n",
            "  done\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "!cat scripts/train_lda_bert.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBs8LT-LFoZk"
      },
      "outputs": [],
      "source": [
        "!sh scripts/train_lda_bert.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71oYJb327Hcj"
      },
      "source": [
        "# Plot attention or compute statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjKeS8NG9gYZ"
      },
      "outputs": [],
      "source": [
        "!cat scripts/inspect_result_lda_bert.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcMPD-T9D3TN"
      },
      "outputs": [],
      "source": [
        "!cat src/inspect_result.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Lj05qr50Y-"
      },
      "outputs": [],
      "source": [
        "!sh scripts/inspect_result_lda_bert.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDg2yq8jOw3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}